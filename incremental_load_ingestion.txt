Incremental Load
"import datetime
import logging
import sys 
import boto3
import json

MSG_FORMAT      = '%(asctime)s %(levelname)s %(name)s: %(message)s'
DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S'
logging.basicConfig(format=MSG_FORMAT, datefmt=DATETIME_FORMAT)
logger          = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

env                     = dbutils.widgets.get('env')
pharma_oraenv           = dbutils.widgets.get('pharma_oraenv')
pharmadb_secret         = dbutils.widgets.get('pharmadb_secret')
region_name             = dbutils.widgets.get('region_name')
pharma_oradriver        = dbutils.widgets.get('pharma_oradriver')
pharma_oraport          = dbutils.widgets.get('pharma_oraport')
pharma_orahost          = dbutils.widgets.get('pharma_orahost')
catalog                 = dbutils.widgets.get('catalog')
cap_rpt_schema          = dbutils.widgets.get('cap_rpt_schema')
cap_raw_schema          = dbutils.widgets.get('cap_raw_schema')

tablelist               = ['frmlry_vrsn_ndc_mv'] 

class CAPSchemaUtilis(object):
    """"""main Class""""""
    def __init__(self):
        """"""
        Constructor for the class `CAPSchemaUtilis`
        """"""
        self.env = env
            
    def db2_jdbc_connection(self):
        try:
            jdbcurl=""jdbc:oracle:thin:@//{pharma_orahost}:{pharma_oraport}/{pharma_oraenv}"".format(pharma_orahost=pharma_orahost,pharma_oraport=pharma_oraport,pharma_oraenv=pharma_oraenv)
            logger.info('Pharma ORA JDBC_URL ==> '+jdbcurl.__str__())
            secrets_client = boto3.client(service_name='secretsmanager',region_name=region_name)
            secret = json.loads(secrets_client.get_secret_value(SecretId=pharmadb_secret).get('SecretString'))
            return jdbcurl,secret['user'],secret['password']

        except Exception as error:
            logger.error('Pharma ORA JDBC Connection Failed for Host ==>\n'+pharma_orahost+':'+pharma_oraport+' \n'+'Error==>'+error.__str__())
            sys.exit(1)
    
    def table_export(self):
        
        for tablename in tablelist:
            try:
                logger.info('Job Start Time ==> '+datetime.datetime.now().__str__())
                logger.info('Job Start for tablename ==> '+tablename.__str__())
                
                jdbcurl,pharma_dbuser,pharma_bdpwd=self.db2_jdbc_connection()
                logger.info('jdbcurl           ==> '+jdbcurl)
                logger.info('pharma_dbuser     ==> '+pharma_dbuser)
                logger.info('pharma_oradriver  ==> '+pharma_oradriver)
                pharmatable='phrmrdsd_o.'+tablename
                
                #drop_query=""DROP TABLE IF EXISTS {catalog}.{cap_raw_schema}.{tablename}"".format(catalog=catalog,cap_raw_schema=cap_raw_schema,tablename=tablename)
                #spark.sql(drop_query)

                #delete_query=""TRUNCATE TABLE {catalog}.{cap_raw_schema}.phrmrdsd_o_frmlry_vrsn_ndc_mv"".format(catalog=catalog,cap_raw_schema=cap_raw_schema)
                #spark.sql(delete_query)
                
                query = """"""select count(1) from {catalog}.{cap_raw_schema}.phrmrdsd_o_frmlry_vrsn_ndc_mv""""""
                query = query.format(catalog=catalog,cap_raw_schema=cap_raw_schema)  
                result = spark.sql(query)
                totalcnt = result.first()[0] 
                logger.info('Total Records ==> ' +totalcnt.__str__())

                if totalcnt==0:
                    query=""""""select *from {pharmatable}"""""".format(pharmatable=pharmatable)
                    fetchsize=200000
                else:
                    max_trxdate=spark.sql('select max(src_sys_updt_dt) max_trxdate from {catalog}.{cap_raw_schema}.{tablename}'.format(catalog=catalog,cap_raw_schema=cap_raw_schema,tablename=tablename)).first()['max_trxdate']
                    logger.info('max_trxdate ==> '+max_trxdate.__str__())
                    query=""""""select *from %s where TO_CHAR(src_sys_updt_dt,'YYYY-mm-dd HH:mi:ss')>'%s'""""""%(pharmatable,max_trxdate)
                    fetchsize=50000
                
                logger.info('Query  ==>\n' +query.__str__())
                #query='select *from {pharmatable} where transctn_dt>{max_trxdate}'.format(pharmatable=pharmatable,max_trxdate=max_trxdate)                
        
                df = spark.read\
                    .format(""jdbc"")\
                    .option(""user"",pharma_dbuser)\
                    .option(""password"",pharma_bdpwd)\
                    .option(""driver"", pharma_oradriver)\
                    .option(""url"",jdbcurl)\
                    .option(""query"", query)\
                    .option(""fetchsize"", fetchsize)\
                    .load()                
                
                #df.persist()
                reccnt=df.count()
                logger.info('Total Records for Insert ==>\n' +reccnt.__str__())

                finaltable=catalog+"".""+cap_raw_schema+"".""+tablename
                #schema=spark.table(finaltable).schema
                df.write.mode('append').format('delta').option('mergeSchema','true').saveAsTable(finaltable)
                #df.write.mode('append').format('delta').option(schema,schema).saveAsTable(finaltable)
                #df.unpersist()
                logger.info('Job End Time ==> '+datetime.datetime.now().__str__())
                
            except Exception as error:
                logger.error(
                    'export failure for table ==>\n' + tablename)
                logger.error(
                    'export failure for table ==>\n' + error.__str__())
    
def main():
    """"""Main Function""""""
    logger.info('Job Start Time ==> '+datetime.datetime.now().__str__())
    ext_utils = CAPSchemaUtilis()
    ext_utils.table_export()
    logger.info('Job End Time ==> '+datetime.datetime.now().__str__())

main()"