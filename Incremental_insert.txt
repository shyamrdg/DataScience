Incremental Insert
import sys
import datetime
import logging
MSG_FORMAT = '%(asctime)s %(levelname)s %(name)s: %(message)s'
DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S'
logging.basicConfig(format=MSG_FORMAT, datefmt=DATETIME_FORMAT)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

env                = dbutils.widgets.get('env')
catalog            = dbutils.widgets.get('catalog')
cap_raw_schema     = dbutils.widgets.get('cap_raw_schema') 


class CAPSchemaUtilis(object):
    """"""main Class""""""
    def __init__(self):
        """"""
        Constructor for the class `CAPSchemaUtilis`
        """"""
        self.env = env  
           
    def insert_overwrite_table(self):
        try:
            
            #query=""""""TRUNCATE TABLE {catalog}.{cap_raw_schema}.phrmrdsd_o_frmlry_vrsn_ndc_mv""""""
            #query = query.format(catalog=catalog,cap_raw_schema=cap_raw_schema)  
            #result = spark.sql(query)
            
            query = """"""select count(*) from {catalog}.{cap_raw_schema}.phrmrdsd_o_frmlry_vrsn_ndc_mv""""""
            query = query.format(catalog=catalog,cap_raw_schema=cap_raw_schema)  
            result = spark.sql(query)
            totalcnt = result.first()[0] 
            logger.info('Total Records ==>\n' +totalcnt.__str__())
            
            if totalcnt>0:
                query = """"""Insert into {catalog}.{cap_raw_schema}.phrmrdsd_o_frmlry_vrsn_ndc_mv
                select acct.*,current_timestamp() ingest_timestamp,current_timestamp() incr_ingest_timestamp 
                from {catalog}.{cap_raw_schema}.frmlry_vrsn_ndc_mv acct
                where acct.src_sys_updt_dt>(select max(src_sys_updt_dt) from {catalog}.{cap_raw_schema}.phrmrdsd_o_frmlry_vrsn_ndc_mv)""""""
            else:
                query = """"""Insert into {catalog}.{cap_raw_schema}.phrmrdsd_o_frmlry_vrsn_ndc_mv
                select acct.*,current_timestamp() ingest_timestamp,current_timestamp() incr_ingest_timestamp 
                from {catalog}.{cap_raw_schema}.frmlry_vrsn_ndc_mv acct
                """"""   

            logger.error('Query ==>\n' +query)
            query = query.format(catalog=catalog,cap_raw_schema=cap_raw_schema)  
            result = spark.sql(query)
            logger.info('insert overwrite result Number of rows inserted for phrmrdsd_o_frmlry_vrsn_ndc_mv==> ' + result.first()['num_inserted_rows'].__str__())
            # logger.info('insert overwrite result Number of rows inserted for phrmrdsd_o_frmlry_vrsn_ndc_mv==> ' + result.first()['num_affected_rows'].__str__())
        except Exception as error:
           msg='Table creation error'+'=>'+'phrmrdsd_o_frmlry_vrsn_ndc_mv'
           logger.error('insert_table Exception for ==>\n' + msg.__str__())
           logger.error('insert_table Exception for ==>\n' + error.__str__())
           sys.exit(1)        

def main():
    """"""Main Function""""""
    logger.info('Job Start Time ==> '+datetime.datetime.now().__str__())
    ext_utils = CAPSchemaUtilis()
    ext_utils.insert_overwrite_table()
    logger.info('Job End Time ==> '+datetime.datetime.now().__str__())

main()